\section*{Related Work}


\begin{description}
    \item[Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey] \hfill \\ This gives a good overview for all the relevant methods that are around in 2019. Also small section for different learning schemas namely, supervised, semi-supervised and unsupervised (and particular here is self-supervised) \cite{jing2019selfsupervised}
    \item[Emerging Properties in Self-Supervised Vision Transformers - DINO] \hfill \\Dino als student-teacher framework. student trys to outperform teacher. after each iteration student gets reinterpreted.\cite{caron2021emerging}
    \item[Unsupervised Semantiv Segmentation By Destilling Feature Correspondences - STEGO] \hfill \\Sits on top of a backbone. Is a contrastive loss algorithm with three parts. To cluster features self similarity, KNN-image similarity and random image similarity gets calculated to push the features appart or together. \cite{hamilton2022unsupervised}
    \item[CUTS: A Fully Unsupervised Framework for Medical Image Segmentation] \hfill \\Contrastive and Unsupervised Training for Segmentation for medical Image Segmentation. Unsupervised manner. After creating a convolutional Patch Embedding the Patches get attracted or pushed apart with contrastive learing and a local patch reconstructionfrom the patch embeding gets formed. Both together for the total Loss function. (similar/dissimilar)patches get calculated via proximity and SSIM metric \cite{liu2023cuts}
    \item[Invariant Information Clustering for Unsupervised Image Classification and Segmentation] \hfill \\ IIC, cluster any kind of unlabelled paired data by training a network to predict cluster identities. This can be applied to image clustering or segmentation by generating the required paired data using random transformations and spatial proximity \cite{ji2019invariant}
    \item[PiCIE: Unsupervised Semantic Segmentation using Invariance and Equivariance in Clustering] \hfill \\Unsupervised Semantic Segmentation using Invariance and Equivariance in Clustering. This means two views get produced per image these get clustered and a cross-view is trained. Invariance is towards photometic transformations and equivariance to geometric transformations. -> Photometric augmentation include changes to the pixel values such as contrast, sharpness, blurring, brightness and color changes. Geometric augmentation include transformations such as cropping, rotation, scaling and translation. Structural augmentation would, in the medical domain, include things like adding tissue and artifacts that could occur during the recording (such as tubes, bias fields, white lines, etc). It is less common for general computer vision problems, but an approach like cutout could be seen as structural augmentation. \cite{cho2021picie}
    \item [DFF] \hfill \\Deep feature factorization for concept discovery. An Image is getting decomposed into semantic regions, represented by both spatial and saliency heatmap by factorizing CNN activations. Basis vectors serve as descroptors for the particular regions. This is getting combined to form a global image descriptor.\cite{collins2018deep}
    \item [MAE] \hfill \\ Masked Autoencoders Are Scalable Vision Learners. Masked autoencoder (MAE) is a simple autoencoding approach that reconstructs the original signal given its partial observation. The encoder part maps the observation into a latent representation and the decoder part reconstructs the original signal from the laten representation space. This approach adopts a asymmetric design that allows the encoder to operate only on the partial, observed signal and a lightweight decoder that reconstricts the full signal from the latent representation and mask tokens. \cite{he2021masked}
    \item[Unsupervised Learning of Visual Features by Contrasting Cluster Assignments] \hfill \\this is for multicroptraining \cite{NEURIPS2020_70feb62b}
    \item[Momentum Contrast for Unsupervised Visual Representation Learning] \hfill\\ -> exponential moving average (EMA).Originally the momentum encoder has been introduced as a substitute for a queue in contrastive learning. However, in the DINO Framework \cite{caron2021emerging}, itsrole differs since they do not have a queue nor a contrastive loss, and may be closer to the role of the mean teacher used in self-training. \cite{he2020momentum} 
    \item [An image is worth 16x16 words: Transformers for image recognition at scale] \hfill \\ this is a Vision Transformer. They split an image into fixed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classification, we use the standard approach of adding an extra learnable “classification token” to the sequence. The illustration of the Transformer encoder was inspired by\cite{NIPS2017_3f5ee243}. Paper is at:\cite{dosovitskiy2021image}
    \item [Contrastive Learning with Stronger Augmentations] \hfill \\ This paper is trying to impove on existing contrastive learing algorithms by implementing stronger augmentations. Conclusion is that the proposed method can utilize the distributional divergence to learn information from strongly augmented images. \cite{wang2022contrastive}
    \item [Dimensionality Reduction by Learning an Invariant Mapping] \hfill \\ First core idea of puling the positive pairs together and pushing the negative pairs away in the embedding space\cite{hadsell2006dimensionality}
    \item [Bootstrap Your Own Latent. A New Apporach To Self-Supervised Learning]\hfill \\ In this approach, two networks get trained, one gets trained with an augmented view that will predict the other networks representation of the same image under a different augmented view. At the same time, the target network gets updated with a slow-moving average of the start network. This networkstructure is very close to the DINO Framework. \cite{grill2020bootstrap}
    \item [SegSort: Segmentation by Discriminative Sorting of Segments] \hfill \\ This approach partitions an image in the embedding space into aligned segments. Additionally it will assign the majority labels from retrived segments. This is an end-to-end framework that consits of a CNN/FCN/PSNet which creates pixel-wise embedings, a clustering method that paritions the embedings into a segmentation and a metric learning formulation for seperating and grouping the segments into semantic clusters.  \cite{hwang2019segsort}
    \item[Uncovering the Inner Workings of STEGO for Safe Unsupervised Semantic Segmentation] This is a followup to STEGO to  
\end{description}



\textcolor{red}{AUS EXPOSE}


Maschinelles Lernen kann in die zwei Bereiche, überwachtes und un-überwachtes Lernen eingeteilt werden. Der Grundgedanke dieser Methodiken beinhaltet, dass Daten in ein Modell gespeist werden, dann mittels einer bestimmten Methodik trainiert und validiert werden um dann ein für das Problem relevantes Ergebnis zu erzielen. \cite{long2019}
Für das überwachte Lernen bedeutet dies, dass ein Teil der Daten Annotiert werden muss. Diese Annotierten Daten werden für das Training und die Validierung genutzt. Für ein sehr hoch spezialisierten Bereich, wie er hier gegeben ist, muss für den Prozess der Annotation ein großes Maß an Expertise vorhanden sein. 
Für das un-überwachte Lernen ist dies nicht von Relevanz, da Annotationen für den Trainingsprzess nötig sind. Lediglich für eine Validierung beziehungsweise eine empirische Auswertung sind annotierte Daten von nutzen. Dieser erhebliche Vorteil macht das un-überwachte Lernen besonders Attraktiv für Einsatzbereiche in welchem große Datenmengen vorliegen oder eine hohe Expertise für die Annotation eines Datums beötigt wird.
Für das Problem in diesem Fall, liegen die Daten in Form von Photolumineszenz-Spektroskopie Bilddaten vor. \cite{pl_spectro_1997}
Die Modelle arbeiten somit immer mit Bilddateien, welche segmentiert werden. Dies bedeutet, dass einzelne Bildausschnitte in verschiedene Klassen eingeteilt werden. 
Dieses Problem wird als Bild segmentations Problem bezeichnet. Diese Problematik lässt sich mit vielen verschiedenen Methodiken lösen. 


Ein sehr angesehenes Modell ist das U-Net \cite{Ronneberger.18.05.2015} welches eine Encoder-Decoder Struktur besitzt. Dies bedeutet, dass erst eine Reduktion der Dimension zur Eigenschaften der Daten zu komprimieren und dann eine Erweiterung des Latenten Raumes zur Lokalisierung der gewonnenen Eigenschaften. Daraus folgend werden hierarchische Features klassiert. Die Reduktion beziehungsweise respektive die Erweiterung der Dimensionalität wird über Faltungs-Schichten realisiert. Aus diesem Grund kann man dieses Netz den Encoder-Decoder basierten Convolutional Neural Network (CNN) zuordnen.

Eine Erweiterung der beschriebenen CNNs sind die Klasse der Fully Connected Networks (FCN). Ein wesentlicher Unterschied  ist, dass die einzelnen Knoten einer Schicht mit jedem Knoten der vorherigen und nachfolgenden Schicht verbunden ist. Jede Verbindung kann dabei ein individuelles Gewicht zu jedem Neuron der nächsten Schicht halten. 

Ein anderer Ansatz wird in der Transformer Architekturen angewendet.\cite{transformer_vaswani_2023}
Diese basiert auf parallelen Multi-Head Aufmerksamkeitskarten. 
Die Transformer Achitektur wurde ursprünglich für das Natural Language Processing (NLP) eingeführt, aber 2020 \cite{DERT_carion_2020} auch als Vision Transformer (ViT eingeführt. 
Transformer bestehen aus einer Encoder Struktur. Dieser enthält eine Multi-Head Self Attention Schicht, eine Multi-Layer Perceptrons Schicht und eine Schicht in der Normalisiert wird. Diese drei Schichten werden in einen Block zusammengeführt. Damit dies auf Bilddaten angewendet werden kann, muss diese Struktur erweitert werden.  

Ein Viion Transformer durchläuft dabei sieben verschiedene Stufen. Zunächst wird das Bild in gleich große Felder aufgeteilt und hintereinander gereiht. Von diesen Bildausschnitten werden lineare Einbettungen erzeugt. Diese werden dann mit der Position des Patches verknüpft. Nach diesem Schritt können nun die einzelnen Teilbilder in den Encoder Block geführt werden. Aus diesem kann dann die Aufmerksamkeit der einzelnen Bildausschnitte abgeleitet werden. Da die verschiedenen Aufmerksamkeits-Karten sehr stark mit einzelnen Objekten in einem Bild korrelieren, können Aufmerksamkeits-Karten auch als einzelne Objekt Klassen gesehen werden. Dieser Ansatz der für Vision Transformer wurde erstmals 2020 von Dosovitskiy et. al. entwickelt. \cite{DosovitskiyTransformer2020}  Heute ist diese Architektur der State of the Art in der Bild-Segmentierung

Invariant Information Clustering \cite{iic_ji_2018} versucht hingegen die gemeinsamme Information zwischen einem Bild und einer zufällig veränderten Version des Bildes zu maximieren. Das in diesem Ansatz verwendete CNN, wandelt das Bild in einen Vektor mithilfe einer Softmax-Funktion um. Jedes Element des berechneten Vektors besteht enthält dabei die Wahrscheinlichkeit einer der möglichen verfügbaren Zielklassen einzuteilen. Durch die Veränderung des Bildes werden somit nur geometrische und photometrische Eigenschaften und nicht der eigentliche Inhalt des Bildes verändert und gefiltert. 

Ein anderer Ansatz verfolgt die Architektur von Caron et. al. 2019 \cite{deepClustering_caron_2019} mit Deep Clustering. Das itarative clustering von tief liegenden Eigenschaften und das Nutzen von Clusterzuordnungen als pseude-Labels um Parameter für das verwendete Netzwerk zu learnen liegt hier im Zentrum dieses Ansatzes. Als Clustering wird der k-means Clustering Algorithmus genutzt.
Das bedeutet, dass immer abwechselnd erst aus den Netzwerk Cluster gebildet werden und diese Pseudolabels wieder zurück in das Netztwerk geführt wird. Diese Schritte werden dann so lange wiederholt bis ein genug gutes Ergebnis erzielt wird.

Ein weiterer geeigneter Ansatz könnte Segmente Anything \cite{sam2023} von Kirillov u.a.  sein. 
Dieser stützt sich auf die Encoder-Decoder Architektur. Dabei stützt der Encoder auf einen Vision Transformer \cite{DosovitskiyTransformer2020} welcher mittels eines Masked Autoencoder \cite{He.11.11.2021} vortrainiert wurde. 
Der Decoder Block besteht aus einem modifizierten Vision Transformer Decoder. Dieser verwendet eine Eigen- und Quer-Aufmerksamkeit. Zuletzt werden die Daten dann mit einem mehrlagigen Perzeptron auf einen dynamischen linearen Klassifizierer übertragen. Dieser berechnet die Wahrscheinlichkeit, ob ein Pixel zum Vordergrund oder Hintergrund gehört.     
Dieses Netzwerk hat große Popularität errungen, weil es eine sehr gute Übertragbarkeit auf neue Daten hat. Ein großen Einfluss, dass dies möglich ist, hat der verwendete Datensatz, mit welchem das Segment Anything Model trainiert ist. Der Datensatz enthält 11 Millionen Bilder mit 1,1 Milliarden Annotationen auf Pixel Ebene. Dieser Datensatz hat mit durchschnittlich 3300 x 4950 Pixeln pro Bild eine sehr hohe Auflösung.  
